{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# pandas imports\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "# machine learning imports\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import TransformerMixin\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn import metrics\n",
    "\n",
    "# display setup\n",
    "pd.set_option(\"display.max_columns\", None) # the None parameter displays unlimited columns\n",
    "sns.set(style=\"whitegrid\") # for plots"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Getting the Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# read the csv file\n",
    "df = pd.read_csv(r\"hotel_bookings.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# display the first 5 rows for a quick look\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# DataFrame shape (rows, columns)\n",
    "# understand the amount of data we are working with\n",
    "df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# description of data\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> In a first observation it is clear that some features have\n",
    "> missing values (i.e. \"company\" and \"agent\" columns).\n",
    "> We will need to take care of this later."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# summary of the numerical attributes\n",
    "# null values are ignored\n",
    "df.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### Features in the DataFrame:\n",
    ">> 1. hotel: Resort Hotel or City Hotel\n",
    ">> 2. is_canceled: Value indicating if the booking was canceled (1) or not (0)\n",
    ">> 3. lead_time: Number of days between the booking date to the arrival date\n",
    ">> 4. arrival_date_year: Year of arrival date\n",
    ">> 5. arrival_date_month: Month of arrival date\n",
    ">> 6. arrival_date_week_number: Week number according to year of arrival\n",
    ">> 7. arrival_date_day_of_month: Day of arrival date\n",
    ">> 8. stays_in_weekend_nights: Number of weekend nights booked (Saturday or Sunday)\n",
    ">> 9. stays_in_week_nights: Number of week nights booked (Monday to Friday)\n",
    ">> 10. adults: Number of adults\n",
    ">> 11. children: Number of children\n",
    ">> 12. babies: Number of babies\n",
    ">> 13. meal: Type of meal booked\n",
    ">> 14. country: Country of origin\n",
    ">> 15. market_segment: Market segment designation, typically influences the price sensitivity\n",
    ">> 16. distribution_channel: Booking distribution channel, refers to how the booking was made\n",
    ">> 17. is_repeated_guest: Value indication if booking name was from a repeated guest (1) or not (0)\n",
    ">> 18. previous_cancellations: Number of previous cancellations prior to current booking\n",
    ">> 19. previous_bookings_not_canceled: Number of previous booking not canceled prior to current booking\n",
    ">> 20. reserved_room_type: Code of room type reserved\n",
    ">> 21. assigned_room_type: Code for the type of room assigned to the booking\n",
    ">> 22. booking_changes: Number of changes made to the booking since entering the hotel management system\n",
    ">> 23. deposit_type: Type of deposit made for the reservation\n",
    ">> 24. agent: ID of the travel agency that made the booking\n",
    ">> 25. company: ID of the company/organization that made the booking or is responsible for payment\n",
    ">> 26. days_in_waiting_list: Number of days booking was in the waiting list until it was confirmed\n",
    ">> 27. customer_type: Type of booking\n",
    ">> 28. adr: Average Daily Rate (the sum of transactions divided by the number of nights stayed)\n",
    ">> 29. required_car_parking_spaces: Number of car parking spaces requested\n",
    ">> 30. total_of_special_requests: Number of special requests made by the customer\n",
    ">> 31. reservation_status: Last reservation status (Canceled, Check-Out, No-Show)\n",
    ">> 32. reservation_status_date: Date at which the last status was set\n",
    ">>\n",
    ">>> ##### *Understanding the features could help gain insight on how to treat null values.*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# a histogram plot for each numerical attribute\n",
    "df.hist(bins=50, figsize=(20,15))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Initial observations from the histograms:\n",
    ">> 1. Some weeks have more bookings. This could be because of holiday or summer seasons, when people tend to travel more.\n",
    ">> 2. According to the lead_time plot most bookings were made shortly before arrival.\n",
    ">> 3. Booking tend to be without children or babies.\n",
    ">> 4. It seems that the most accommodations are two weeks long or shorter.\n",
    ">> 5. While most bookings were not canceled, there are thousands of instances that were."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> # Objective:\n",
    "> ## Predicting if a booking will be canceled.\n",
    ">> ### Chosen Feature:\n",
    ">> #### *is_canceled* column\n",
    ">>> 0 means the booking was not canceled\n",
    ">>>\n",
    ">>> 1 means the booking was canceled\n",
    ">> ### Motive:\n",
    ">> Like any business, hotels are also looking to gain profit. A model that predicts if the booking\n",
    ">> is likely to be canceled could be a good indication for hotels, as they\n",
    ">> may prefer to accept the lower risk bookings first."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### Splitting the Data:\n",
    ">> Before further analysis let's split the data into a training set and a testing set.\n",
    ">> This will ensure avoidance of bias that could occur from learning the data as a whole."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# use sklearn train_test_split function to split the data\n",
    "# the reason for selecting 0.15 as the test size is because the dataset is very large\n",
    "# the random state parameter ensures that data will be shuffled and split the same way in each run\n",
    "train_set, test_set = train_test_split(df, test_size=0.15, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Number of instances in training set: \", len(train_set))\n",
    "print(\"Number of instances in testing set: \", len(test_set))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Understanding and Visualizing the Data\n",
    "> ##### *The motivation for this section is to gain more insights.*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# deep copy of the training set\n",
    "df2 = train_set.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df2.head(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### Missing Features:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# the methods below calculate the number of missing values\n",
    "missing_values = df2.isna().sum()\n",
    "missing_values = missing_values[missing_values != 0]\n",
    "missing_values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# replace missing values\n",
    "\n",
    "# can assume that there were no children\n",
    "df2.fillna({\"children\": 0}, inplace=True)\n",
    "\n",
    "# missing countries can be labeled unknown\n",
    "df2.fillna({\"country\": \"Unknown\"}, inplace=True)\n",
    "\n",
    "# missing agent ID can be zero, likely booked privately\n",
    "df2.fillna({\"agent\": 0}, inplace=True)\n",
    "\n",
    "# missing company ID can be zero, likely personal booking\n",
    "df2.fillna({\"company\": 0}, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check that the values were filled\n",
    "df2.isna().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### Numerical Attributes:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# method creates a correlations matrix\n",
    "corr_matrix = df2.corr()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# looking at attributes correlation with is_canceled feature\n",
    "corr_matrix[\"is_canceled\"].sort_values(ascending=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# experimenting with attribute combinations\n",
    "\n",
    "# adds column with total amount of guests that stayed\n",
    "df2[\"guests_stayed\"] = df2[\"adults\"] + df2[\"children\"] + df2[\"babies\"]\n",
    "\n",
    "# adds column with total nights stayed\n",
    "df2[\"nights_stayed\"] = df2[\"stays_in_week_nights\"] + df2[\"stays_in_weekend_nights\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# looking at the correlation matrix again with the added columns\n",
    "corr_matrix = df2.corr()\n",
    "corr_matrix[\"is_canceled\"].sort_values(ascending=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### Correlations with is_canceled Attribute - Overview:\n",
    "> The strongest positive correlations (0.1 or more) are:\n",
    "> * lead_time\n",
    "> * previous_cancellations\n",
    ">\n",
    "> The strongest negative correlations (-0.1 or less) are:\n",
    "> * total_of_special_requests\n",
    "> * required_car_parking_spaces\n",
    "> * booking_changes\n",
    ">\n",
    "> The attribute combinations tested (guests stayed and nights stayed) both had weak correlations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### Cancellations According to Lead Time"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# density plot of lead time\n",
    "# shows the distribution and highest concentration points\n",
    "plt.figure(figsize=(10,5))\n",
    "lead_time = df2['lead_time']\n",
    "lead_time = pd.DataFrame(sorted(lead_time, reverse = True), columns = ['Lead'])\n",
    "sns.distplot(lead_time)\n",
    "plt.title(\"Lead Time\", size=20)\n",
    "plt.xlabel(\"lead time days\", size=15)\n",
    "plt.ylabel(\"density\", size=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# divides lead time by less than 100 days, 100-355 days and 365 or more days\n",
    "lead_time_1 = df2[df2[\"lead_time\"] < 100]\n",
    "lead_time_2 = df2[(df2[\"lead_time\"] >= 100) & (df2[\"lead_time\"] < 365)]\n",
    "lead_time_3 = df2[df2[\"lead_time\"] >= 365]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# calculates cancellations according to lead time groups\n",
    "lead_cancel_1 = lead_time_1[\"is_canceled\"].value_counts()\n",
    "lead_cancel_2 = lead_time_2[\"is_canceled\"].value_counts()\n",
    "lead_cancel_3 = lead_time_3[\"is_canceled\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# density plot for each lead time group\n",
    "fig, bx = plt.subplots(1,3,figsize=(21,6))\n",
    "sns.distplot(lead_time_1[\"lead_time\"], ax = bx[0])\n",
    "bx[0].set_title(\"lead_time [0,100) days\", size=20)\n",
    "sns.distplot(lead_time_2[\"lead_time\"], ax = bx[1])\n",
    "bx[1].set_title(\"lead_time [100,365) days\", size=20)\n",
    "sns.distplot(lead_time_3[\"lead_time\"], ax = bx[2])\n",
    "bx[2].set_title(\"lead_time [365,max) days\", size=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# total count of lead time according to cancellation\n",
    "total_lead_days_cancel = pd.DataFrame(data=[lead_cancel_1,lead_cancel_2,lead_cancel_3],\n",
    "             index=[\"[0,100) days\", \"[100,365) days\", \"[365,max) days\"])\n",
    "total_lead_days_cancel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize=(21,6))\n",
    "ax[0].pie(np.array([total_lead_days_cancel[0][0], total_lead_days_cancel[1][0]]),\n",
    "          labels=[\"not_canceled\", \"canceled\"], autopct='%1.1f%%', startangle=90,\n",
    "          colors=['forestgreen', 'firebrick'])\n",
    "ax[0].set_title(\"lead_time [0,100) days\", size=20)\n",
    "ax[1].pie(np.array([total_lead_days_cancel[0][1], total_lead_days_cancel[1][1]]),\n",
    "          labels=[\"not_canceled\", \"canceled\"], autopct='%1.1f%%', startangle=90,\n",
    "          colors=['forestgreen', 'firebrick'])\n",
    "ax[1].set_title(\"lead_time [100,365) days\", size=20)\n",
    "ax[2].pie(np.array([total_lead_days_cancel[0][2], total_lead_days_cancel[1][2]]),\n",
    "          labels=[\"not_canceled\", \"canceled\"], autopct='%1.1f%%', startangle=90,\n",
    "          colors=['forestgreen', 'firebrick'])\n",
    "ax[2].set_title(\"lead_time [365,max) days\", size=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> #### Observations:\n",
    ">> * Most bookings occur about 5 days prior to arrival.\n",
    ">> * When the lead time is larger the chances for cancellation increase.\n",
    ">> * The amount of bookings is steady overall between 20-100 days, then drops."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### Cancellations According to Previous Cancellations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# gets previous cancellation column\n",
    "prev_cancel = df2[\"previous_cancellations\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# sorts index values\n",
    "prev_cancel.value_counts().sort_index()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Cancellation Rates:\\n\")\n",
    "print('Never canceled =' ,str(round(df2[df2['previous_cancellations']==0]\n",
    "                                            ['is_canceled'].mean()*100,2))+' %')\n",
    "print('Canceled once =' ,str(round(df2[df2['previous_cancellations']==1]\n",
    "                                            ['is_canceled'].mean()*100,2))+' %')\n",
    "print('Canceled more than 10 times:',str(round(df2[df2['previous_cancellations']>10]\n",
    "                                            ['is_canceled'].mean()*100,2))+' %')\n",
    "print('Canceled more than 11 times:' ,str(round(df2[df2['previous_cancellations']>11]\n",
    "                                            ['is_canceled'].mean()*100,2))+' %')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# creates a list with previous cancellations indices\n",
    "prev_cancel_index = df2[\"previous_cancellations\"].value_counts().index.to_list()\n",
    "# sorts the list\n",
    "prev_cancel_index.sort()\n",
    "\n",
    "# calculates the average percentage of cancellations for each value in DataFrame\n",
    "percentage_prev_cancel= []\n",
    "for i in prev_cancel_index:\n",
    "    percentage_prev_cancel.append((round(df2[df2[\"previous_cancellations\"]==i]\n",
    "                                        [\"is_canceled\"].mean()*100,2)))\n",
    "percentage_prev_cancel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# creates a DataFrame with the results\n",
    "df_prev_cancel = pd.DataFrame(percentage_prev_cancel, index=prev_cancel_index, columns=[\"Previous Cancellations %\"])\n",
    "df_prev_cancel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plots previous cancellations by percentages\n",
    "df_prev_cancel.plot(figsize= (10,5))\n",
    "plt.title(\"Previous Cancellations\", size=20)\n",
    "plt.xlabel(\"Number of Previous Cancellations\", size=15)\n",
    "plt.ylabel(\"%\", size=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### Observations:\n",
    ">> The percentages show that when there are more previous cancellations, there is\n",
    ">> a substantially higher chance the customer will cancel again."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### Cancellations According to Total of Special Requests"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# number of instances for each value\n",
    "df2[\"total_of_special_requests\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# group by cancellations\n",
    "is_canceled = df2.groupby(by=\"is_canceled\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get groups according to binary outcomes\n",
    "canceled = is_canceled.get_group(1)\n",
    "not_canceled = is_canceled.get_group(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# counts values for each outcome\n",
    "special_requests_0 = not_canceled[\"total_of_special_requests\"].value_counts()\n",
    "special_requests_1 = canceled[\"total_of_special_requests\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# creates a DataFrame for each outcome\n",
    "df_special_requests_0 = pd.DataFrame(special_requests_0.values, index=special_requests_0.index,\n",
    "                                     columns=[\"not_canceled\"])\n",
    "df_special_requests_1 = pd.DataFrame(special_requests_1.values, index=special_requests_1.index,\n",
    "                                     columns=[\"canceled\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# joins both DataFrames side by side\n",
    "df_special_requests= df_special_requests_0.join(df_special_requests_1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# adds total of both outcomes\n",
    "special_requests_total = df_special_requests[\"not_canceled\"] + df_special_requests[\"canceled\"]\n",
    "\n",
    "# calculates percentage of cancellations for each number of requests value individually\n",
    "special_requests_percentage = []\n",
    "for i in special_requests_total.index:\n",
    "    special_requests_percentage.append(round((special_requests_1[i]/special_requests_total[i])*100,2))\n",
    "special_requests_percentage"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# add percentages as new column in DataFrame\n",
    "df_special_requests.join(pd.DataFrame(special_requests_percentage, index=df_special_requests.index,\n",
    "             columns=[\"cancellations %\"]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plots special requests according to cancellations\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.countplot(x=df2[\"total_of_special_requests\"], hue=df2[\"is_canceled\"])\n",
    "plt.title(\"Special Requests\", size=20)\n",
    "plt.xlabel(\"Number of Special Requests\", size=15)\n",
    "plt.legend([\"not canceled\", \"canceled\"])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### Observations:\n",
    ">> * Nearly half of the bookings without special requests are canceled.\n",
    ">> * There are fewer cancellations when the number of special requests increases."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### Cancellations According to Required Car Parking Spaces"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df2[\"required_car_parking_spaces\"].value_counts().sort_index()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# counts values for each outcome\n",
    "parking_spaces_0 = not_canceled[\"required_car_parking_spaces\"].value_counts()\n",
    "parking_spaces_1 = canceled[\"required_car_parking_spaces\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# value counts for non canceled instances\n",
    "parking_spaces_0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# value counts for canceled instances\n",
    "parking_spaces_1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# pie plot of cancellations with zero required parking spaces\n",
    "plt.pie(x=[parking_spaces_0[0], parking_spaces_1[0]], labels=[\"not_canceled\", \"canceled\"],\n",
    "        autopct='%1.1f%%', startangle=90, colors=['forestgreen', 'firebrick'])\n",
    "plt.title(\"Zero Required Parking Spaces Cancellations\", size=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### Observations:\n",
    ">> * Dividing the instances into groups according to cancellations shows that all canceled\n",
    ">> bookings were ones without required parking spaces.\n",
    ">> * This could potentially be a bad indication for cancellations. The model could learn\n",
    ">> that a booking can be canceled **only** if no parking spaces were required, which does not\n",
    ">> necessarily have to be the case."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### Cancellations According to Booking Changes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df2[\"booking_changes\"].value_counts().sort_index()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# counts values for each outcome\n",
    "booking_changes_0 = not_canceled[\"booking_changes\"].value_counts()\n",
    "booking_changes_1 = canceled[\"booking_changes\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# sorts index numbers by value\n",
    "booking_changes_0.sort_index()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# sorts index numbers by value\n",
    "booking_changes_1.sort_index()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_booking_changes_1 = pd.DataFrame(booking_changes_1, index=booking_changes_0.index)\n",
    "df_booking_changes_1.fillna({\"booking_changes\": 0}, inplace=True)\n",
    "booking_changes_1 = pd.Series(df_booking_changes_1[\"booking_changes\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# adds total of both outcomes\n",
    "booking_changes_total = booking_changes_0 + booking_changes_1\n",
    "\n",
    "# calculates percentage of cancellations for each number of booking changes individually\n",
    "percentage_booking_changes = []\n",
    "for i in booking_changes_total.index:\n",
    "    percentage_booking_changes.append(round((booking_changes_1[i]/booking_changes_total[i])*100,2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create a DataFrame with the percentage of cancellations\n",
    "df_percentage_booking_changes = pd.DataFrame(percentage_booking_changes, index=booking_changes_total.index,\n",
    "                                             columns=[\"cancellations %\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# creates a DataFrame for each outcome\n",
    "df_booking_changes_0 = pd.DataFrame(booking_changes_0.values, index=booking_changes_0.index,\n",
    "             columns=[\"not_canceled\"])\n",
    "df_booking_changes_1 = pd.DataFrame(booking_changes_1.values, index=booking_changes_1.index,\n",
    "             columns=[\"canceled\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# joins all three DataFrames side by side\n",
    "df_booking_changes = df_booking_changes_0.join\\\n",
    "    ([df_booking_changes_1, df_percentage_booking_changes])\n",
    "\n",
    "# remove rows with 0% cancellations\n",
    "df_booking_changes = df_booking_changes[df_booking_changes[\"cancellations %\"]!=0]\n",
    "df_booking_changes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### Observations:\n",
    ">> * While a large amount of bookings with no changes were canceled, this category can change overtime\n",
    ">> which could possibly be a source of leakage."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### ADR\n",
    ">"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df2[df2[\"adr\"]==0][\"guests_stayed\"].value_counts().sort_index()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df2[df2[\"adr\"]==0][\"reservation_status\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df2[\"adr\"].sort_values()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### Categorical Attributes:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### Cancellations According to Hotels and Arrival Month"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df2[\"hotel\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# a plot of the number of instances for each hotel according to cancellations\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.countplot(x=df2[\"hotel\"], hue=df2[\"is_canceled\"])\n",
    "plt.title(\"Hotel Cancellations\", size=20)\n",
    "plt.legend([\"not canceled\", \"canceled\"])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ordered_months = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n",
    "          \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
    "\n",
    "resort_canceled_percent = []\n",
    "city_canceled_percent = []\n",
    "\n",
    "# dividing cancellation outcome by hotel and month of arrival\n",
    "resort_1 = canceled[canceled[\"hotel\"]==\"Resort Hotel\"][\"arrival_date_month\"].value_counts()\n",
    "resort_0 = not_canceled[not_canceled[\"hotel\"]==\"Resort Hotel\"][\"arrival_date_month\"].value_counts()\n",
    "city_1 = canceled[canceled[\"hotel\"]==\"City Hotel\"][\"arrival_date_month\"].value_counts()\n",
    "city_0 = not_canceled[not_canceled[\"hotel\"]==\"City Hotel\"][\"arrival_date_month\"].value_counts()\n",
    "\n",
    "# calculating cancellation percentage according to hotel\n",
    "for i in ordered_months:\n",
    "    resort_canceled_percent.append(round((resort_1[i] / (resort_0[i]+resort_1[i]))*100,2))\n",
    "    city_canceled_percent.append(round((city_1[i]/(city_0[i]+city_1[i]))*100,2))\n",
    "\n",
    "# creates a DataFrame with the cancellation percentage of each hotel\n",
    "df_resort_cancel = pd.DataFrame(resort_canceled_percent, index=ordered_months,\n",
    "                                       columns=[\"Resort Hotel Canceled %\"])\n",
    "df_city_cancel = pd.DataFrame(city_canceled_percent, index=ordered_months,\n",
    "                                       columns=[\"City Hotel Canceled %\"])\n",
    "\n",
    "# joins DataFrames\n",
    "df_hotel_cancel = df_resort_cancel.join(df_city_cancel)\n",
    "df_hotel_cancel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### Observations:\n",
    ">> * There are more instances for City Hotel than Resort Hotel in the data.\n",
    ">> * City Hotel has a higher cancellation rate according to arrival months."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### Cancellations According to Meal Booked"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plots meal according to cancellations\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.countplot(x=df2[\"meal\"], hue=df2[\"is_canceled\"])\n",
    "plt.title(\"Cancellations According to Meal Booked\", size=20)\n",
    "plt.xlabel(\"meal\", size=15)\n",
    "plt.legend([\"not canceled\", \"canceled\"])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### Observations:\n",
    ">> * The BB (Bed & Breakfast) meal is most common. It is also most frequently canceled."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### Cancellations According to Market Segment, Distribution Channel, Customer Type and Room Type"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# using groupby to show onl non canceled instances\n",
    "reserved_room_1 = not_canceled[[\"reserved_room_type\", \"market_segment\", \"customer_type\",\n",
    "                            \"distribution_channel\", \"adr\", \"guests_stayed\"]]\n",
    "\n",
    "# remove instances without guests\n",
    "reserved_room_1 = reserved_room_1.loc[(reserved_room_1[\"guests_stayed\"]>0)]\n",
    "# calculate average adr per guest\n",
    "# dividing adr by guests stayed\n",
    "reserved_room_1[\"adr_per_guest\"] = reserved_room_1[\"adr\"] / reserved_room_1[\"guests_stayed\"]\n",
    "\n",
    "# plot of adr according to market segment and room type\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=reserved_room_1[\"market_segment\"], y= reserved_room_1[\"adr_per_guest\"],\n",
    "            hue=reserved_room_1[\"reserved_room_type\"])\n",
    "plt.title(\"ADR According to Market Segment and Room Type\", size=20)\n",
    "plt.legend(loc=2)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df2[\"market_segment\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# calculate cancellation percentage according to market segment\n",
    "market_segment_percent = []\n",
    "\n",
    "market_segment_1 = canceled[\"market_segment\"].value_counts()\n",
    "market_segment_total = df2[\"market_segment\"].value_counts()\n",
    "\n",
    "for i in market_segment_total.index:\n",
    "    market_segment_percent.append(str(i+\": \") +\n",
    "                    str(round((market_segment_1[i]/market_segment_total[i])*100,2)))\n",
    "market_segment_percent"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df2[\"distribution_channel\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# calculate cancellation percentage according to distribution channel\n",
    "distribution_channel_percent = []\n",
    "\n",
    "distribution_channel_1 = canceled[\"distribution_channel\"].value_counts()\n",
    "distribution_channel_total = df2[\"distribution_channel\"].value_counts()\n",
    "\n",
    "for i in distribution_channel_total.index:\n",
    "    distribution_channel_percent.append(str(i+\": \") +\n",
    "                    str(round((distribution_channel_1[i]/distribution_channel_total[i])*100,2)))\n",
    "distribution_channel_percent"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df2[\"customer_type\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# calculate cancellation percentage according to customer type\n",
    "customer_type_percent = []\n",
    "\n",
    "customer_type_1 = canceled[\"customer_type\"].value_counts()\n",
    "customer_type_total = df2[\"customer_type\"].value_counts()\n",
    "\n",
    "for i in customer_type_total.index:\n",
    "    customer_type_percent.append(str(i+\": \") +\n",
    "                    str(round((customer_type_1[i]/customer_type_total[i])*100,2)))\n",
    "customer_type_percent"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot of cancellations according to room type\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.countplot(x=df2[\"reserved_room_type\"], hue=df2[\"is_canceled\"])\n",
    "plt.title(\"Cancellations According to Room Type\", size=20)\n",
    "plt.legend([\"not canceled\", \"canceled\"], loc=1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### Observations:\n",
    ">> * Aviation (airline staff) have the highest adr (relative cost of a room when the booking would be made for).\n",
    ">> * Market segment cancellation rates are highest amongst travel agencies and tour operators.\n",
    ">> * Distribution channel cancellation rates are highest amongst groups, travel agencies and tour operators.\n",
    ">> * Customer type cancellation rates are highest amongst transient\n",
    ">> (meaning the booking is not part of a group or contract and is not associated to another transient booking).\n",
    ">> * The room type \"A\" is canceled most frequently."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### Cancellations According to Deposit Type"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df2[\"deposit_type\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# calculate deposit type instances percentage in data\n",
    "deposit_percent = round(df2[\"deposit_type\"].value_counts()/len(df[\"deposit_type\"])*100,4)\n",
    "deposit_percent"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# using groupby to divide according to deposit types\n",
    "deposit = df2.groupby(by=\"deposit_type\")\n",
    "non_refund = deposit.get_group(\"Non Refund\")\n",
    "refundable = deposit.get_group(\"Refundable\")\n",
    "no_deposit = deposit.get_group(\"No Deposit\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# calculate number of cancellations according to deposit type\n",
    "no_deposit_0 = (no_deposit[\"is_canceled\"]==0).sum()\n",
    "no_deposit_1 = (no_deposit[\"is_canceled\"]==1).sum()\n",
    "non_refund_0 = (non_refund[\"is_canceled\"]==0).sum()\n",
    "non_refund_1 = (non_refund[\"is_canceled\"]==1).sum()\n",
    "refundable_0 = (refundable[\"is_canceled\"]==0).sum()\n",
    "refundable_1 = (refundable[\"is_canceled\"]==1).sum()\n",
    "all_canceled = no_deposit_1 + non_refund_1 + refundable_1\n",
    "all_not_canceled = no_deposit_0 + non_refund_0 + refundable_0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check that all values were calculated\n",
    "all_canceled + all_not_canceled == df2[\"deposit_type\"].size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create a DataFrame with the number of instances for each deposit type\n",
    "df_deposit_type = pd.DataFrame(index=[\"Not Canceled\", \"Canceled\"])\n",
    "df_deposit_type[\"no_deposit\"] = [no_deposit_0, no_deposit_1]\n",
    "df_deposit_type[\"non_refund\"] = [non_refund_0, non_refund_1]\n",
    "df_deposit_type[\"refundable\"] = [refundable_0, refundable_1]\n",
    "df_deposit_type"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cancel_labels = [\"cancelled\", \"not Cancelled\"]\n",
    "fig, dx = plt.subplots(1,3, figsize=(21,6))\n",
    "dx[0].pie(np.array([no_deposit_1, no_deposit_0]), labels=cancel_labels,\n",
    "            autopct='%1.1f%%', startangle=90, colors=['firebrick', 'forestgreen'])\n",
    "dx[0].set_title(\"No Deposit Cancellations\", size=20)\n",
    "dx[1].pie(np.array([non_refund_1, non_refund_0]), labels=cancel_labels,\n",
    "            autopct='%1.1f%%', startangle=90, colors=['firebrick', 'forestgreen'])\n",
    "dx[1].set_title(\"Non Refund Cancellations\", size=20)\n",
    "dx[2].pie(np.array([refundable_1, refundable_0]), labels=cancel_labels,\n",
    "            autopct='%1.1f%%', startangle=90, colors=['firebrick', 'forestgreen'])\n",
    "dx[2].set_title(\"Refundable Cancellations\", size=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> #### Observations:\n",
    ">> * The non refund values and graph looks a bit off. It almost seems as if the values\n",
    ">> for cancellation were switched!\n",
    ">> In light of this, it might be better to evaluate the model both with and without this\n",
    ">> feature."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### Cancellations According to Country of Origin"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df2[\"country\"].unique().size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "canceled[\"country\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# calculate countries by number of instances that appear in data\n",
    "country_1 = (df2[\"country\"].value_counts() <= 1).sum()\n",
    "country_10 = (df2[\"country\"].value_counts() <= 10).sum()\n",
    "country_50 = (df2[\"country\"].value_counts() <= 50).sum()\n",
    "country_100 = (df2[\"country\"].value_counts() <= 100).sum()\n",
    "country_1000 = (df2[\"country\"].value_counts() <= 1000).sum()\n",
    "\n",
    "print(\"Number of countries with one or less instances:\", country_1,\n",
    "      \"\\nNumber of countries with 10 or less instances:\", country_10,\n",
    "      \"\\nNumber of countries with 50 or less instances:\", country_50,\n",
    "      \"\\nNumber of countries with 100 or less instances:\", country_100,\n",
    "      \"\\nNumber of countries with 1000 or less instances:\", country_1000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### Observations:\n",
    ">> * There are 175 unique countries. This indicates that the data is representative\n",
    ">> worldwide, contrary to a specific region.\n",
    ">> * More than half of the instances have 50 or fewer observations in the DataFrame.\n",
    ">> * A model would likely generalize better if we avoid using this column."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Data Cleaning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# clean copy of training set\n",
    "df3 = train_set.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# removes instances with zero guests\n",
    "\n",
    "class RemoveZeroGuests(TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        XData = X.loc[((X[\"adults\"]) + (X[\"children\"]) + (X[\"babies\"])) > 0]\n",
    "        return XData"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df3.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# use class to remove instances with zero guests that stayed\n",
    "df3 = RemoveZeroGuests().fit_transform(df3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df3.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# separate predictors from target values\n",
    "\n",
    "# drop creates a copy without changing the training set\n",
    "X_train = df3.drop(\"is_canceled\", axis=1)\n",
    "\n",
    "# create a deep copy of the target values\n",
    "y_train = df3[\"is_canceled\"].copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### Removing the Following Columns:\n",
    ">> #### Numerical Attributes:\n",
    ">> * arrival_date_year: This category references towards certain years. This could be\n",
    ">> problematic for instances during years that do not appear in the training data, or\n",
    ">> perhaps have bias towards certain years specifically due to different amounts in the\n",
    ">> training data.\n",
    ">> * arrival_date_day_of_month: The column arrival date week of month generalizes this.\n",
    ">> * booking_changes: Could change over time, potentially causing data leakage.\n",
    ">> * days_in_waiting_list: Could constantly change over time. Additionally, there are many\n",
    ">> instances. This could prevent the model from generalizing.\n",
    ">> * agent & company: Represented by an ID. These columns are uninformative since they\n",
    ">> contain a substantial amount of various numerical values without having an actual\n",
    ">> numerical meaning. Since other columns (such as market segment) indicate the type of\n",
    ">> reservation, these columns won't be of much additional use.\n",
    ">>\n",
    ">> #### Categorical Attributes:\n",
    ">> * country: There are many categories, most with few instances. In order to make a model\n",
    ">> that generalizes, it is better to dismiss this category.\n",
    ">> * assigned_room_type: Similar to reserved_room_type and seems like the reserved room is\n",
    ">> a more suitable choice.\n",
    ">> * reservation_status: Major data leakage! The categories are Check-Out, Canceled and No-Show.\n",
    ">> This is exactly what we are trying to predict.\n",
    ">> * reservation_status_date: This is the date when the reservation status was last changed,\n",
    ">> and therefore is irrelevant."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_features = [\"lead_time\", \"stays_in_weekend_nights\", \"stays_in_week_nights\", \"adults\",\n",
    "                \"children\", \"babies\", \"is_repeated_guest\", \"previous_cancellations\",\n",
    "                \"previous_bookings_not_canceled\", \"adr\", \"required_car_parking_spaces\",\n",
    "                \"total_of_special_requests\"]\n",
    "\n",
    "cat_features = [\"hotel\", \"arrival_date_week_number\", \"arrival_date_month\", \"meal\",\n",
    "                \"market_segment\", \"distribution_channel\", \"reserved_room_type\",\n",
    "                \"deposit_type\", \"customer_type\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Undefined/SC both represent no meal package and can be combined\n",
    "\n",
    "class ReplaceMeal(TransformerMixin):\n",
    "\n",
    "    def fit(self,X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        XData = X.copy()\n",
    "        XData[\"meal\"].replace(\"Undefined\", \"SC\", inplace=True)\n",
    "        return XData"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# SimpleImputer constant default fills values with zero\n",
    "# MinMaxScaler normalize data (rescale between 0-1)\n",
    "num_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\")),\n",
    "    (\"min_max\", MinMaxScaler())\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# SimpleImputer fills missing values with 'Unknown'\n",
    "# OneHotEncoder converts categories to a numeric dummy array\n",
    "# (one binary attribute per category)\n",
    "cat_pipeline = Pipeline([\n",
    "    (\"meal\", ReplaceMeal()),\n",
    "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"Unknown\")),\n",
    "    (\"one_hot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# column transformer:\n",
    "# features generated by each transformer will be concatenated to form a single feature space\n",
    "# columns of the original feature matrix that are not specified are dropped\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"numerical\", num_pipeline, num_features),\n",
    "    (\"categorical\", cat_pipeline, cat_features)\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# transform training data using pipeline\n",
    "X_train_prepared = full_pipeline.fit_transform(X_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Training and Evaluating Models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Accuracy is less relevant for an imbalanced classification problem.\n",
    "Evaluating by a metric that represents the data better is important."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# function prints scores, mean and std\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def display_evaluation(train, pred):\n",
    "    print(\"Accuracy:\", metrics.accuracy_score(train, pred))\n",
    "    print(\"Confusion Matrix:\\n\", metrics.confusion_matrix(train, pred))\n",
    "    print(\"Precision:\", metrics.precision_score(train, pred))\n",
    "    print(\"Recall:\", metrics.recall_score(train, pred))\n",
    "    print(\"F1 Score:\", metrics.f1_score(train, pred))\n",
    "    print(\"ROC AUC Score:\", metrics.roc_auc_score(train, pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### KNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# instantiate classifier\n",
    "# default k=5\n",
    "knn = KNeighborsClassifier()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# train the model on the training set\n",
    "knn.fit(X_train_prepared, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# test on a few instances from training data\n",
    "some_data = X_train.iloc[:10]\n",
    "some_labels = y_train.iloc[:10]\n",
    "some_data_prepared = full_pipeline.transform(some_data)\n",
    "print(\"Predictions:\", knn.predict(some_data_prepared))\n",
    "print(\"Labels:\", list(some_labels))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "knn_pred_1 = knn.predict(X_train_prepared)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display_evaluation(y_train, knn_pred_1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# instantiate KNN model using distance instead of uniform\n",
    "# distance means closer instances have a larger weight\n",
    "# uniform weighs all instances equally\n",
    "# default k=5\n",
    "knn = KNeighborsClassifier(weights=\"distance\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "knn.fit(X_train_prepared, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0 0 0 0 0 0 1 1 1 0]\n",
      "Labels: [0, 0, 0, 0, 0, 0, 1, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# test on a few instances from training data\n",
    "some_data = X_train.iloc[:10]\n",
    "some_labels = y_train.iloc[:10]\n",
    "some_data_prepared = full_pipeline.transform(some_data)\n",
    "print(\"Predictions:\", knn.predict(some_data_prepared))\n",
    "print(\"Labels:\", list(some_labels))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "knn_pred_2 = knn.predict(X_train_prepared)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9895289508225843\n",
      "Confusion Matrix:\n",
      " [[63548   372]\n",
      " [  689 36718]]\n",
      "Precision: 0.9899703424103532\n",
      "Recall: 0.9815809875157057\n",
      "F1 Score: 0.98575781575097\n",
      "ROC AUC Score: 0.9878806063986538\n"
     ]
    }
   ],
   "source": [
    "display_evaluation(y_train, knn_pred_2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> So far, the performance of the KNN model using distance weights instead of uniform\n",
    "> drastically improved the results.\n",
    ">\n",
    "> We will test on another model and look for the best hyperparameters for most promising\n",
    "> model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "> ### What is the Random Forest Classification Model?\n",
    "\n",
    "Forests are based on multiple decision trees, so it is vital to first understand how decision\n",
    "trees work.\n",
    "\n",
    "A decision tree is a non-linear model built by constructing many linear boundaries.\n",
    "The tree works as a sequence of yes or no, true or false questions that progress down\n",
    "the tree until reaching a predicted class. The data is split into nodes based on\n",
    "feature values. This model is good for occasions when there is no single linear line that\n",
    "can divide the data. Gini Impurity of a node represents the probability that a randomly chosen\n",
    "sample would be incorrectly classified, so the goal is to reduce this as much as possible.\n",
    "\n",
    "Using a single decision tree could cause overfitting of the training data. For example,\n",
    "a decision tree could create a leaf node (the predicted class) for each instance.\n",
    "Using a forest could help generalize better to new data. The random forest model\n",
    "samples random point and subsets of features when training. Then, the predictions are made\n",
    "by averaging the predictions of each decision tree."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (1SA-Final-Project)",
   "language": "python",
   "name": "pycharm-6b1cb0de"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}